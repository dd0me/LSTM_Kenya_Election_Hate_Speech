
+import re
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout
+from tensorflow.keras.preprocessing.sequence import pad_sequences
+from tensorflow.keras.models import Sequential
+from tensorflow.keras.preprocessing.text import one_hot
+from nltk.stem.porter import PorterStemmer
+from nltk.corpus import stopwords
+import matplotlib.pyplot as plt
+import seaborn as sns
+import string
+import nltk
+from nltk.corpus import stopwords
+import seaborn as sns
+
+#Read file
+ds=pd.read_excel("path/path/path")
+#Drop unwanted
+ds=ds.drop(['Text_Resized', 'Embedded URLs',
+       'Actual_URLs', 'Subject', 'Date', 'Month',
+       'Entity',
+       'Account_Holder_Profile_Genuine','Category' ] ,axis = 1)
+#after dropping null values,indexes will be unordered therfore resetting indexes
+ds.reset_index(inplace = True,drop = True)
+#Bar plots to check label distribution month on month
+forBarGraph=pd.crosstab(ds['Month words'], ds['Label'])
+#  Bar plot
+import seaborn
+seaborn.barplot(x=ds['Month words'], y=ds['Likes'],data=ds)
+seaborn.barplot(x=ds['Label'], y=ds['Month words'],data=ds)
+# Stacked bar chart.
+forBarGraph.plot(kind='bar', stacked=True)

+# Just add a title and rotate the x-axis labels to be horizontal.
+plt.title('True and False Label stacking')
+plt.xticks(rotation=0, ha='center')
+ds2=ds.iloc[:,0:1]
+#Separate x and y variables for training and prediction
+x=ds.iloc[:, 1:2]
+y = ds['Label']
+#checking the distribution of real and fake news labels
+sns.countplot(x = 'Label',data = ds)
+#Text Cleaning and preprocessing

+cleaned = []
+for i in range(0,len(ds)):
    
+    #removing words any other than (a-z) and (A-Z)
+    text = re.sub('[^a-zA-Z]',' ', x['Text_resized_2'][i])
    
+    #converting all words into lower case
+    text = text.lower()
    
+    #tokenizing 
+    text = text.split()
    
+    #stemming and removing stopwords
+    ps = PorterStemmer()
+    text = [ps.stem(words) for words in text if words not in stopwords.words('english')]
+    text = ' '.join(text)
+    cleaned.append(text)
+#taking dictionary size 5000
+vocab_size = 5000

+#one hot encoding
+one_hot_dir = [one_hot(words,vocab_size) for words in cleaned]

+#length of all rows should be equal therefore applying padding
+#this will adjust size by adding 0 at staring of the shorter rows
+embedded_layer = pad_sequences(one_hot_dir,padding = 'pre')
+embedded_layer
+#converting into numpy arrays.
+x = np.array(embedded_layer)
+y = np.array(y)
+#splitting the Dataset into Train and Test set
+from sklearn.model_selection import train_test_split
+x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
+#Add optimizers
+#from tensorflow import keras
+#from tensorflow.keras import layers
+#Import optimizers
+from keras import optimizers

+#creating model using LSTM
+model = Sequential()
+#model = keras.Sequential()

+#taking number features as 50
+model.add(Embedding(vocab_size,50,input_length = len(embedded_layer[0])))
+model.add(Dropout(0.5))

+#adding LSTM layers with 100 neurons
+model.add(LSTM(100))
+model.add(Dropout(0.5))

+#adding output layer 
+model.add(Dense(1,activation="sigmoid"))
+#model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))
+#model.add(layers.Activation('softmax'))

+#compiling the model
+model.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])
+#opt = keras.optimizers.Adam(learning_rate=0.01)
+#model.compile(loss='categorical_crossentropy', optimizer=opt)
+
+#summary of model
+model.summary()
+
+#training the model
+model.fit(x_train, y_train, validation_data = (x_test,y_test), epochs = 11, batch_size = 256, validation_split=0.2)
+
+#predicting and getting accuracy
+y_pred = model.predict(x_test)
+y_pred = (y_pred > 0.5)
+
+from sklearn.metrics import accuracy_score
+accuracy_score(y_test,y_pred)
+
+#getting confusion matrix
+from sklearn.metrics import confusion_matrix
+confusion_matrix(y_test,y_pred)
